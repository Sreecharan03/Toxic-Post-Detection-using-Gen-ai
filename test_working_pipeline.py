#!/usr/bin/env python3
"""
Complete Twitter Toxicity Detection Test
Uses working HTTP client + toxicity detection pipeline
"""

import os
import sys
import asyncio
import logging
from datetime import datetime

# Proper directory path setup
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.join(current_dir, 'src'))

from twitter_parser import TwitterURLParser
from simple_twitter_client import SimpleTwitterClient
from toxic_detector import ToxicityDetector

def setup_logging():
    """Setup logging"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

async def test_complete_pipeline():
    """Test the complete pipeline with working HTTP client"""
    
    print("ğŸš€ TWITTER TOXICITY DETECTION - COMPLETE TEST")
    print("=" * 60)
    
    # The URL that we know works
    test_url = "https://x.com/rahulroushan/status/2001161855340003510?s=20"
    
    print(f"ğŸ“ Testing URL: {test_url}")
    print(f"ğŸ“… Test Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("-" * 60)
    
    try:
        # Step 1: Initialize components
        print("\nğŸ” STEP 1: INITIALIZING COMPONENTS")
        print("-" * 40)
        
        url_parser = TwitterURLParser()
        twitter_client = SimpleTwitterClient('/teamspace/studios/this_studio/.env')
        toxicity_detector = ToxicityDetector('/teamspace/studios/this_studio/.env')
        
        print("âœ… URL Parser initialized")
        print("âœ… HTTP Twitter Client initialized") 
        print("âœ… Toxicity Detector initialized")
        
        # Step 2: Parse URL
        print("\nğŸ” STEP 2: URL PARSING")
        print("-" * 40)
        
        tweet_id = url_parser.extract_tweet_id(test_url)
        if tweet_id:
            print(f"âœ… Tweet ID extracted: {tweet_id}")
        else:
            print("âŒ Failed to extract tweet ID")
            return
        
        # Step 3: Fetch Tweet
        print("\nğŸ” STEP 3: TWEET FETCHING (HTTP METHOD)")
        print("-" * 40)
        
        tweet_data = twitter_client.get_tweet_by_id(tweet_id)
        
        if tweet_data:
            print("âœ… Tweet fetched successfully!")
            print(f"ğŸ‘¤ Author: @{tweet_data.author_username}")
            print(f"ğŸ“… Created: {tweet_data.created_at}")
            print(f"ğŸ“ Text: {tweet_data.text}")
            
            if tweet_data.public_metrics:
                metrics = tweet_data.public_metrics
                print(f"ğŸ“Š Engagement:")
                print(f"   ğŸ’™ Likes: {metrics.get('like_count', 0)}")
                print(f"   ğŸ”„ Retweets: {metrics.get('retweet_count', 0)}")
                print(f"   ğŸ’¬ Replies: {metrics.get('reply_count', 0)}")
        else:
            print("âŒ Failed to fetch tweet")
            return
        
        # Step 4: Toxicity Analysis
        print("\nğŸ” STEP 4: DUAL-LAYER TOXICITY ANALYSIS")
        print("-" * 40)
        
        print(f"ğŸ”„ Analyzing text: '{tweet_data.text}'")
        
        toxicity_result = await toxicity_detector.analyze_text(tweet_data.text)
        
        print(f"\nâœ… Analysis Complete!")
        print(f"ğŸ¯ Overall Toxicity Score: {toxicity_result.overall_score:.3f}")
        print(f"ğŸ”’ Confidence Level: {toxicity_result.confidence:.3f}")
        
        # Step 5: Results Breakdown
        print(f"\nğŸ“Š CATEGORY BREAKDOWN:")
        for category, score in toxicity_result.categories.items():
            status = "ğŸ”´" if score > 0.5 else "ğŸŸ¡" if score > 0.2 else "ğŸŸ¢"
            print(f"   {status} {category.capitalize()}: {score:.3f}")
        
        print(f"\nğŸ¤– AI ANALYSIS:")
        if toxicity_result.explanation:
            print(f"   ğŸ’­ Explanation: {toxicity_result.explanation}")
        
        if toxicity_result.reformulation:
            print(f"   âœ¨ Suggested Improvement: {toxicity_result.reformulation}")
        
        print(f"\nğŸ”¬ LAYER PERFORMANCE:")
        ml_max = max(toxicity_result.layer1_scores.values()) if toxicity_result.layer1_scores else 0
        gemini_max = max(toxicity_result.layer2_scores.values()) if toxicity_result.layer2_scores else 0
        print(f"   ğŸ¤– ML Layer (BERT): {ml_max:.3f}")
        print(f"   ğŸ§  Gemini Layer: {gemini_max:.3f}")
        
        # Step 6: Final Assessment
        print(f"\nğŸ“‹ FINAL ASSESSMENT:")
        toxicity_level = "HIGH" if toxicity_result.overall_score >= 0.7 else "MEDIUM" if toxicity_result.overall_score >= 0.3 else "LOW"
        print(f"   ğŸ¯ Toxicity Level: {toxicity_level}")
        print(f"   ğŸš¨ Action Needed: {'YES' if toxicity_result.overall_score >= 0.5 else 'NO'}")
        
        # Step 7: Technical Summary
        print(f"\n" + "=" * 60)
        print("ğŸ‰ COMPLETE PIPELINE TEST SUCCESSFUL!")
        print("=" * 60)
        
        print(f"âœ… Components Working:")
        print(f"   â€¢ URL Parsing: PASS")
        print(f"   â€¢ HTTP Twitter API: PASS") 
        print(f"   â€¢ Dual-Layer Toxicity Detection: PASS")
        print(f"   â€¢ AI Explanations: PASS")
        print(f"   â€¢ Score Fusion: PASS")
        print(f"   â€¢ Confidence Calculation: PASS")
        
        print(f"\nğŸ† B.Tech Project Status: FULLY OPERATIONAL")
        print(f"ğŸ“Š Ready for demonstration and presentation!")
        
    except Exception as e:
        print(f"âŒ Pipeline Error: {str(e)}")
        import traceback
        traceback.print_exc()

async def main():
    """Main function"""
    setup_logging()
    
    print("ğŸ¯ TWITTER TOXICITY DETECTION SYSTEM")
    print("ğŸ”¬ B.Tech Final Year Project")
    print("ğŸ‘¨â€ğŸ’» Complete Pipeline Test")
    print("=" * 60)
    
    await test_complete_pipeline()

if __name__ == "__main__":
    asyncio.run(main())